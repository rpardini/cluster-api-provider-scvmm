Bugs:
- Not sure, but the namepool could get out of sync with actual state.
  It should probably update the owned list when the resource gets updated, simply by scanning all the
  scvmmmachines in the same namespace, just to be sure.
- Cosmetic fix: Switch from 'start' and 'end' to 'from' and 'to', just so they appear in alphabetical order?
- Sometimes an scvmmmachine claims two names in a namepool.
  Would be fixed if the generatevmname function would check if the 'owner' already has a name in the
  status list, and then just return that.

Todo:
- ScvmmNamePool should probably have a webhook that checks for overlapping ranges

- Handle crashing out while creating vms and stuff (currently it could try to create the same VM twice)

- Add metrics, specifically how long WinRM calls take, maybe some more generic things
  (Also maybe log how long each call takes?)

- Find a way to relate log lines to each other, if there are multiple concurrent reconciliations.

- What should the cluster reconciler do?  Maybe generate a kAPI vip or something?
  . Which responsibilities is the cluster controller supposed to have?
  . Looks like at least setting the ControlPlaneEndpoint correctly
  . Isn't there a chicken-egg issue where the endpoint needs to be known before the first
    control plane has been initialized?
  . Easiest is probably defining it in the spec, and then using that to setup the LB
    But does the kubeadm init need the endpoint?  The kubeadm join certainly does.
  . Should probably use a custom orchestrator script to claim the VIP for the kube-api (new-stipaddress)
  . Claiming the ingress vip should be left to in-cluster
- Does the cluster reconciler also have to apply a CNI networker?
  (Maybe.  There seems to be a 'cni' label on the cluster)
  But now, we apply a CNI using argocd.

- Doing kubernetes-api loadbalancing
  . MetalLB?
  . Keepalived?
  . Choice?

- Regularly check the status of the VMs
  . What to do when somebody turns off the VM externally?
    Currently it will probably recreate the cloud-init and then boot it up again,
    that would surprise some sysadmins
  . Think about error scenarios

- Keep the WinRM powershell running (stop it after a number of seconds of inactivity)
  (Powershell startup and module loading takes a few seconds each time)
  . Probably use a channel to send the commands and get back the results?
  . Needs decent error handling for reconnection and stuff

  * Open global channel for winrm calls
  * A number of worker goroutines, equal to number of concurrent reconciliations setting
  * Send winrm command + scvmmprovider ref + return channel on global winrm channel
  * impl idea: Worker has outer and inner loop.
    - In outer loop it waits for a command on the channel
    - Open and initialize WinRM, defer close, enter inner loop
      - Execute command, return result on return channel
      - Wait for a command on the channel with a timeout
      - If timeout, break loop
      - When scvmmprovider ref doesn't match, break loop but keep command
    But how does that work when there are two scvmmproviders floating around?
    Will probably be fine unless there are reconciliations on both at the same time,
    then it will degrade to closing the connection each time.
    Seems like a rare case, so should be ok for now.

- Better handling of external errors

- Maybe look into setting networking via hyper-v integration instead of cloud-init networkconfig ??

